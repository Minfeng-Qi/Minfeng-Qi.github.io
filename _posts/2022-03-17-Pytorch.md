---
layout: post
title: "Pytorch Tutorial"
date: 2022-03-18
description: "Pytorch Tutorial"
tag: pytorch
---

# [2.2 数据操作](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.2_tensor?id=_22-数据操作)

在深度学习中，我们通常会频繁地对数据进行操作。作为动手学深度学习的基础，本节将介绍如何对内存中的数据进行操作。

在PyTorch中，`torch.Tensor`是存储和变换数据的主要工具。如果你之前用过NumPy，你会发现`Tensor`和NumPy的多维数组非常类似。然而，`Tensor`提供GPU计算和自动求梯度等更多功能，这些使`Tensor`更加适合深度学习。

> "tensor"这个单词一般可译作“张量”，张量可以看作是一个多维数组。标量可以看作是0维张量，向量可以看作1维张量，矩阵可以看作是二维张量。

## [2.2.1 创建`Tensor`](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.2_tensor?id=_221-创建tensor)

我们先介绍`Tensor`的最基本功能，即`Tensor`的创建。

首先导入PyTorch：

```python
import torchCopy to clipboardErrorCopied
```

然后我们创建一个5x3的未初始化的`Tensor`：

```python
x = torch.empty(5, 3)
print(x)Copy to clipboardErrorCopied
```

输出：

```
tensor([[ 0.0000e+00,  1.5846e+29,  0.0000e+00],
        [ 1.5846e+29,  5.6052e-45,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.5846e+29, -2.4336e+02]])Copy to clipboardErrorCopied
```

创建一个5x3的随机初始化的`Tensor`:

```python
x = torch.rand(5, 3)
print(x)Copy to clipboardErrorCopied
```

输出：

```
tensor([[0.4963, 0.7682, 0.0885],
        [0.1320, 0.3074, 0.6341],
        [0.4901, 0.8964, 0.4556],
        [0.6323, 0.3489, 0.4017],
        [0.0223, 0.1689, 0.2939]])Copy to clipboardErrorCopied
```

创建一个5x3的long型全0的`Tensor`:

```python
x = torch.zeros(5, 3, dtype=torch.long)
print(x)Copy to clipboardErrorCopied
```

输出：

```
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])Copy to clipboardErrorCopied
```

还可以直接根据数据创建:

```python
x = torch.tensor([5.5, 3])
print(x)Copy to clipboardErrorCopied
```

输出：

```
tensor([5.5000, 3.0000])Copy to clipboardErrorCopied
```

还可以通过现有的`Tensor`来创建，此方法会默认重用输入`Tensor`的一些属性，例如数据类型，除非自定义数据类型。

```python
x = x.new_ones(5, 3, dtype=torch.float64)  # 返回的tensor默认具有相同的torch.dtype和torch.device
print(x)

x = torch.randn_like(x, dtype=torch.float) # 指定新的数据类型
print(x) Copy to clipboardErrorCopied
```

输出：

```
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
tensor([[ 0.6035,  0.8110, -0.0451],
        [ 0.8797,  1.0482, -0.0445],
        [-0.7229,  2.8663, -0.5655],
        [ 0.1604, -0.0254,  1.0739],
        [ 2.2628, -0.9175, -0.2251]])Copy to clipboardErrorCopied
```

我们可以通过`shape`或者`size()`来获取`Tensor`的形状:

```python
print(x.size())
print(x.shape)Copy to clipboardErrorCopied
```

输出：

```
torch.Size([5, 3])
torch.Size([5, 3])Copy to clipboardErrorCopied
```

> 注意：返回的torch.Size其实就是一个tuple, 支持所有tuple的操作。

还有很多函数可以创建`Tensor`，去翻翻官方API就知道了，下表给了一些常用的作参考。

| 函数                              | 功能                      |
| --------------------------------- | ------------------------- |
| Tensor(*sizes)                    | 基础构造函数              |
| tensor(data,)                     | 类似np.array的构造函数    |
| ones(*sizes)                      | 全1Tensor                 |
| zeros(*sizes)                     | 全0Tensor                 |
| eye(*sizes)                       | 对角线为1，其他为0        |
| arange(s,e,step)                  | 从s到e，步长为step        |
| linspace(s,e,steps)               | 从s到e，均匀切分成steps份 |
| rand/randn(*sizes)                | 均匀/标准分布             |
| normal(mean,std)/uniform(from,to) | 正态分布/均匀分布         |
| randperm(m)                       | 随机排列                  |

这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu)。

## [2.2.2 操作](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.2_tensor?id=_222-操作)

本小节介绍`Tensor`的各种操作。

### [算术操作](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.2_tensor?id=算术操作)

在PyTorch中，同一种操作可能有很多种形式，下面用加法作为例子。

- 加法形式一

  ```python
    y = torch.rand(5, 3)
    print(x + y)Copy to clipboardErrorCopied
  ```

- 加法形式二

  ```python
    print(torch.add(x, y))Copy to clipboardErrorCopied
  ```

  还可指定输出：

  ```python
    result = torch.empty(5, 3)
    torch.add(x, y, out=result)
    print(result)Copy to clipboardErrorCopied
  ```

- 加法形式三、inplace

  ```python
    # adds x to y
    y.add_(x)
    print(y)Copy to clipboardErrorCopied
  ```

  > **注：PyTorch操作inplace版本都有后缀`_`, 例如`x.copy_(y), x.t_()`**

以上几种形式的输出均为：

```
tensor([[ 1.3967,  1.0892,  0.4369],
        [ 1.6995,  2.0453,  0.6539],
        [-0.1553,  3.7016, -0.3599],
        [ 0.7536,  0.0870,  1.2274],
        [ 2.5046, -0.1913,  0.4760]])Copy to clipboardErrorCopied
```

### [索引](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.2_tensor?id=索引)

我们还可以使用类似NumPy的索引操作来访问`Tensor`的一部分，需要注意的是：**索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。**

```python
y = x[0, :]
y += 1
print(y)
print(x[0, :]) # 源tensor也被改了Copy to clipboardErrorCopied
```

输出：

```
tensor([1.6035, 1.8110, 0.9549])
tensor([1.6035, 1.8110, 0.9549])Copy to clipboardErrorCopied
```

除了常用的索引选择数据之外，PyTorch还提供了一些高级的选择函数:

| 函数                            | 功能                                                  |
| ------------------------------- | ----------------------------------------------------- |
| index_select(input, dim, index) | 在指定维度dim上选取，比如选取某些行、某些列           |
| masked_select(input, mask)      | 例子如上，a[a>0]，使用ByteTensor进行选取              |
| nonzero(input)                  | 非0元素的下标                                         |
| gather(input, dim, index)       | 根据index，在dim维度上选取数据，输出的size与index一样 |

这里不详细介绍，用到了再查官方文档。

### [改变形状](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.2_tensor?id=改变形状)

用`view()`来改变`Tensor`的形状：

```python
y = x.view(15)
z = x.view(-1, 5)  # -1所指的维度可以根据其他维度的值推出来
print(x.size(), y.size(), z.size())Copy to clipboardErrorCopied
```

输出：

```
torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])Copy to clipboardErrorCopied
```

**注意`view()`返回的新`Tensor`与源`Tensor`虽然可能有不同的`size`，但是是共享`data`的，也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变)**

```python
x += 1
print(x)
print(y) # 也加了1Copy to clipboardErrorCopied
```

输出：

```
tensor([[1.6035, 1.8110, 0.9549],
        [1.8797, 2.0482, 0.9555],
        [0.2771, 3.8663, 0.4345],
        [1.1604, 0.9746, 2.0739],
        [3.2628, 0.0825, 0.7749]])
tensor([1.6035, 1.8110, 0.9549, 1.8797, 2.0482, 0.9555, 0.2771, 3.8663, 0.4345,
        1.1604, 0.9746, 2.0739, 3.2628, 0.0825, 0.7749])Copy to clipboardErrorCopied
```

所以如果我们想返回一个真正新的副本（即不共享data内存）该怎么办呢？Pytorch还提供了一个`reshape()`可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用`clone`创造一个副本然后再使用`view`。[参考此处](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch)

```python
x_cp = x.clone().view(15)
x -= 1
print(x)
print(x_cp)Copy to clipboardErrorCopied
```

输出:

```
tensor([[ 0.6035,  0.8110, -0.0451],
        [ 0.8797,  1.0482, -0.0445],
        [-0.7229,  2.8663, -0.5655],
        [ 0.1604, -0.0254,  1.0739],
        [ 2.2628, -0.9175, -0.2251]])
tensor([1.6035, 1.8110, 0.9549, 1.8797, 2.0482, 0.9555, 0.2771, 3.8663, 0.4345,
        1.1604, 0.9746, 2.0739, 3.2628, 0.0825, 0.7749])Copy to clipboardErrorCopied
```

> 使用`clone`还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源`Tensor`。

另外一个常用的函数就是`item()`, 它可以将一个标量`Tensor`转换成一个Python number：

```python
x = torch.randn(1)
print(x)
print(x.item())Copy to clipboardErrorCopied
```

输出：

```
tensor([2.3466])
2.3466382026672363Copy to clipboardErrorCopied
```

### [线性代数](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.2_tensor?id=线性代数)

另外，PyTorch还支持一些线性函数，这里提一下，免得用起来的时候自己造轮子，具体用法参考官方文档。如下表所示：

| 函数                              | 功能                              |
| --------------------------------- | --------------------------------- |
| trace                             | 对角线元素之和(矩阵的迹)          |
| diag                              | 对角线元素                        |
| triu/tril                         | 矩阵的上三角/下三角，可指定偏移量 |
| mm/bmm                            | 矩阵乘法，batch的矩阵乘法         |
| addmm/addbmm/addmv/addr/baddbmm.. | 矩阵运算                          |
| t                                 | 转置                              |
| dot/cross                         | 内积/外积                         |
| inverse                           | 求逆矩阵                          |
| svd                               | 奇异值分解                        |

PyTorch中的`Tensor`支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，可参考[官方文档](https://pytorch.org/docs/stable/tensors.html)。

## [2.2.3 广播机制](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.2_tensor?id=_223-广播机制)

前面我们看到如何对两个形状相同的`Tensor`做按元素运算。当对两个形状不同的`Tensor`按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个`Tensor`形状相同后再按元素运算。例如：

```python
x = torch.arange(1, 3).view(1, 2)
print(x)
y = torch.arange(1, 4).view(3, 1)
print(y)
print(x + y)Copy to clipboardErrorCopied
```

输出：

```
tensor([[1, 2]])
tensor([[1],
        [2],
        [3]])
tensor([[2, 3],
        [3, 4],
        [4, 5]])Copy to clipboardErrorCopied
```

由于`x`和`y`分别是1行2列和3行1列的矩阵，如果要计算`x + y`，那么`x`中第一行的2个元素被广播（复制）到了第二行和第三行，而`y`中第一列的3个元素被广播（复制）到了第二列。如此，就可以对2个3行2列的矩阵按元素相加。

## [2.2.4 运算的内存开销](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.2_tensor?id=_224-运算的内存开销)

前面说了，索引操作是不会开辟新内存的，而像`y = x + y`这样的运算是会新开内存的，然后将`y`指向新内存。为了演示这一点，我们可以使用Python自带的`id`函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。

```python
x = torch.tensor([1, 2])
y = torch.tensor([3, 4])
id_before = id(y)
y = y + x
print(id(y) == id_before) # False Copy to clipboardErrorCopied
```

如果想指定结果到原来的`y`的内存，我们可以使用前面介绍的索引来进行替换操作。在下面的例子中，我们把`x + y`的结果通过`[:]`写进`y`对应的内存中。

```python
x = torch.tensor([1, 2])
y = torch.tensor([3, 4])
id_before = id(y)
y[:] = y + x
print(id(y) == id_before) # TrueCopy to clipboardErrorCopied
```

我们还可以使用运算符全名函数中的`out`参数或者自加运算符`+=`(也即`add_()`)达到上述效果，例如`torch.add(x, y, out=y)`和`y += x`(`y.add_(x)`)。

```python
x = torch.tensor([1, 2])
y = torch.tensor([3, 4])
id_before = id(y)
torch.add(x, y, out=y) # y += x, y.add_(x)
print(id(y) == id_before) # TrueCopy to clipboardErrorCopied
```

> 注：虽然`view`返回的`Tensor`与源`Tensor`是共享`data`的，但是依然是一个新的`Tensor`（因为`Tensor`除了包含`data`外还有一些其他属性），二者id（内存地址）并不一致。

## [2.2.5 `Tensor`和NumPy相互转换](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.2_tensor?id=_225-tensor和numpy相互转换)

我们很容易用`numpy()`和`from_numpy()`将`Tensor`和NumPy中的数组相互转换。但是需要注意的一点是： **这两个函数所产生的的`Tensor`和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！**

> 还有一个常用的将NumPy中的array转换成`Tensor`的方法就是`torch.tensor()`, 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的`Tensor`和原来的数据不再共享内存。

### [`Tensor`转NumPy](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.2_tensor?id=tensor转numpy)

使用`numpy()`将`Tensor`转换成NumPy数组:

```python
a = torch.ones(5)
b = a.numpy()
print(a, b)

a += 1
print(a, b)
b += 1
print(a, b)Copy to clipboardErrorCopied
```

输出：

```
tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]
tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]
tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]Copy to clipboardErrorCopied
```

### [NumPy数组转`Tensor`](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.2_tensor?id=numpy数组转tensor)

使用`from_numpy()`将NumPy数组转换成`Tensor`:

```python
import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
print(a, b)

a += 1
print(a, b)
b += 1
print(a, b)Copy to clipboardErrorCopied
```

输出：

```
[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)Copy to clipboardErrorCopied
```

所有在CPU上的`Tensor`（除了`CharTensor`）都支持与NumPy数组相互转换。

此外上面提到还有一个常用的方法就是直接用`torch.tensor()`将NumPy数组转换成`Tensor`，需要注意的是该方法总是会进行数据拷贝，返回的`Tensor`和原来的数据不再共享内存。

```python
c = torch.tensor(a)
a += 1
print(a, c)Copy to clipboardErrorCopied
```

输出

```
[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)Copy to clipboardErrorCopied
```

## [2.2.6 `Tensor` on GPU](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.2_tensor?id=_226-tensor-on-gpu)

用方法`to()`可以将`Tensor`在CPU和GPU（需要硬件支持）之间相互移动。

```python
# 以下代码只有在PyTorch GPU版本上才会执行
if torch.cuda.is_available():
    device = torch.device("cuda")          # GPU
    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor
    x = x.to(device)                       # 等价于 .to("cuda")
    z = x + y
    print(z)
    print(z.to("cpu", torch.double))       # to()还可以同时更改数据类型
```

# [2.3 自动求梯度](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.3_autograd?id=_23-自动求梯度)

在深度学习中，我们经常需要对函数求梯度（gradient）。PyTorch提供的[autograd](https://pytorch.org/docs/stable/autograd.html)包能够根据输入和前向传播过程自动构建计算图，并执行反向传播。本节将介绍如何使用autograd包来进行自动求梯度的有关操作。

## [2.3.1 概念](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.3_autograd?id=_231-概念)

上一节介绍的`Tensor`是这个包的核心类，如果将其属性`.requires_grad`设置为`True`，它将开始追踪(track)在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。完成计算后，可以调用`.backward()`来完成所有梯度计算。此`Tensor`的梯度将累积到`.grad`属性中。

> 注意在`y.backward()`时，如果`y`是标量，则不需要为`backward()`传入任何参数；否则，需要传入一个与`y`同形的`Tensor`。解释见 2.3.2 节。

如果不想要被继续追踪，可以调用`.detach()`将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外，还可以用`with torch.no_grad()`将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数（`requires_grad=True`）的梯度。

`Function`是另外一个很重要的类。`Tensor`和`Function`互相结合就可以构建一个记录有整个计算过程的有向无环图（DAG）。每个`Tensor`都有一个`.grad_fn`属性，该属性即创建该`Tensor`的`Function`, 就是说该`Tensor`是不是通过某些运算得到的，若是，则`grad_fn`返回一个与这些运算相关的对象，否则是None。

下面通过一些例子来理解这些概念。

## [2.3.2 `Tensor`](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.3_autograd?id=_232-tensor)

创建一个`Tensor`并设置`requires_grad=True`:

```python
x = torch.ones(2, 2, requires_grad=True)
print(x)
print(x.grad_fn)Copy to clipboardErrorCopied
```

输出：

```
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
NoneCopy to clipboardErrorCopied
```

再做一下运算操作：

```python
y = x + 2
print(y)
print(y.grad_fn)Copy to clipboardErrorCopied
```

输出：

```
tensor([[3., 3.],
        [3., 3.]], grad_fn=<AddBackward>)
<AddBackward object at 0x1100477b8>Copy to clipboardErrorCopied
```

注意x是直接创建的，所以它没有`grad_fn`, 而y是通过一个加法操作创建的，所以它有一个为`<AddBackward>`的`grad_fn`。

像x这种直接创建的称为叶子节点，叶子节点对应的`grad_fn`是`None`。

```python
print(x.is_leaf, y.is_leaf) # True FalseCopy to clipboardErrorCopied
```

再来点复杂度运算操作：

```python
z = y * y * 3
out = z.mean()
print(z, out)Copy to clipboardErrorCopied
```

输出：

```
tensor([[27., 27.],
        [27., 27.]], grad_fn=<MulBackward>) tensor(27., grad_fn=<MeanBackward1>)Copy to clipboardErrorCopied
```

通过`.requires_grad_()`来用in-place的方式改变`requires_grad`属性：

```python
a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False
a = ((a * 3) / (a - 1))
print(a.requires_grad) # False
a.requires_grad_(True)
print(a.requires_grad) # True
b = (a * a).sum()
print(b.grad_fn)Copy to clipboardErrorCopied
```

输出：

```
False
True
<SumBackward0 object at 0x118f50cc0>Copy to clipboardErrorCopied
```

## [2.3.3 梯度](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.3_autograd?id=_233-梯度)

因为`out`是一个标量，所以调用`backward()`时不需要指定求导变量：

```python
out.backward() # 等价于 out.backward(torch.tensor(1.))Copy to clipboardErrorCopied
```

我们来看看`out`关于`x`的梯度 d(out)dx\frac{d(out)}{dx}*d**x**d*(*o**u**t*):

```python
print(x.grad)Copy to clipboardErrorCopied
```

输出：

```
tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])Copy to clipboardErrorCopied
```

我们令`out`为 oo*o* , 因为

o=14∑4i=1zi=14∑4i=13(xi+2)2o=\frac14\sum_{i=1}^4z_i=\frac14\sum_{i=1}^43(x_i+2)^2*o*=41∑*i*=14*z**i*=41∑*i*=143(*x**i*+2)2

所以

∂o∂xi∣xi=1=92=4.5\frac{\partial{o}}{\partial{x_i}}\bigr\rvert_{x_i=1}=\frac{9}{2}=4.5∂*x**i*∂*o*∣∣*x**i*=1=29=4.5

所以上面的输出是正确的。



数学上，如果有一个函数值和自变量都为向量的函数 y⃗ =f(x⃗ )\vec{y}=f(\vec{x})*y*=*f*(*x*), 那么 y⃗ \vec{y}*y* 关于 x⃗ \vec{x}*x* 的梯度就是一个雅可比矩阵（Jacobian matrix）:

J=⎛⎝⎜⎜⎜⎜∂y1∂x1⋮∂ym∂x1⋯⋱⋯∂y1∂xn⋮∂ym∂xn⎞⎠⎟⎟⎟⎟J=\left(\begin{array}{ccc}\frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{1}}{\partial x_{n}}\\\vdots & \ddots & \vdots\\\frac{\partial y_{m}}{\partial x_{1}} & \cdots & \frac{\partial y_{m}}{\partial x_{n}}\end{array}\right)*J*=⎝⎜⎛∂*x*1∂*y*1⋮∂*x*1∂*y**m*⋯⋱⋯∂*x**n*∂*y*1⋮∂*x**n*∂*y**m*⎠⎟⎞

而`torch.autograd`这个包就是用来计算一些雅克比矩阵的乘积的。例如，如果 vv*v* 是一个标量函数的 l=g(y⃗ )l=g\left(\vec{y}\right)*l*=*g*(*y*) 的梯度：

v=(∂l∂y1⋯∂l∂ym)v=\left(\begin{array}{ccc}\frac{\partial l}{\partial y_{1}} & \cdots & \frac{\partial l}{\partial y_{m}}\end{array}\right)*v*=(∂*y*1∂*l*⋯∂*y**m*∂*l*)

那么根据链式法则我们有 ll*l* 关于 x⃗ \vec{x}*x* 的雅克比矩阵就为:

vJ=(∂l∂y1⋯∂l∂ym)⎛⎝⎜⎜⎜⎜∂y1∂x1⋮∂ym∂x1⋯⋱⋯∂y1∂xn⋮∂ym∂xn⎞⎠⎟⎟⎟⎟=(∂l∂x1⋯∂l∂xn)v J=\left(\begin{array}{ccc}\frac{\partial l}{\partial y_{1}} & \cdots & \frac{\partial l}{\partial y_{m}}\end{array}\right) \left(\begin{array}{ccc}\frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{1}}{\partial x_{n}}\\\vdots & \ddots & \vdots\\\frac{\partial y_{m}}{\partial x_{1}} & \cdots & \frac{\partial y_{m}}{\partial x_{n}}\end{array}\right)=\left(\begin{array}{ccc}\frac{\partial l}{\partial x_{1}} & \cdots & \frac{\partial l}{\partial x_{n}}\end{array}\right)*v**J*=(∂*y*1∂*l*⋯∂*y**m*∂*l*)⎝⎜⎛∂*x*1∂*y*1⋮∂*x*1∂*y**m*⋯⋱⋯∂*x**n*∂*y*1⋮∂*x**n*∂*y**m*⎠⎟⎞=(∂*x*1∂*l*⋯∂*x**n*∂*l*)



注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。

```python
# 再来反向传播一次，注意grad是累加的
out2 = x.sum()
out2.backward()
print(x.grad)

out3 = x.sum()
x.grad.data.zero_()
out3.backward()
print(x.grad)Copy to clipboardErrorCopied
```

输出：

```
tensor([[5.5000, 5.5000],
        [5.5000, 5.5000]])
tensor([[1., 1.],
        [1., 1.]])Copy to clipboardErrorCopied
```

> 现在我们解释2.3.1节留下的问题，为什么在`y.backward()`时，如果`y`是标量，则不需要为`backward()`传入任何参数；否则，需要传入一个与`y`同形的`Tensor`? 简单来说就是为了避免向量（甚至更高维张量）对张量求导，而转换成标量对张量求导。举个例子，假设形状为 `m x n` 的矩阵 X 经过运算得到了 `p x q` 的矩阵 Y，Y 又经过运算得到了 `s x t` 的矩阵 Z。那么按照前面讲的规则，dZ/dY 应该是一个 `s x t x p x q` 四维张量，dY/dX 是一个 `p x q x m x n`的四维张量。问题来了，怎样反向传播？怎样将两个四维张量相乘？？？这要怎么乘？？？就算能解决两个四维张量怎么乘的问题，四维和三维的张量又怎么乘？导数的导数又怎么求，这一连串的问题，感觉要疯掉…… 为了避免这个问题，我们**不允许张量对张量求导，只允许标量对张量求导，求导结果是和自变量同形的张量**。所以必要时我们要把张量通过将所有张量的元素加权求和的方式转换为标量，举个例子，假设`y`由自变量`x`计算而来，`w`是和`y`同形的张量，则`y.backward(w)`的含义是：先计算`l = torch.sum(y * w)`，则`l`是个标量，然后求`l`对自变量`x`的导数。 [参考](https://zhuanlan.zhihu.com/p/29923090)

来看一些实际例子。

```python
x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)
y = 2 * x
z = y.view(2, 2)
print(z)Copy to clipboardErrorCopied
```

输出：

```
tensor([[2., 4.],
        [6., 8.]], grad_fn=<ViewBackward>)Copy to clipboardErrorCopied
```

现在 `z` 不是一个标量，所以在调用`backward`时需要传入一个和`z`同形的权重向量进行加权求和得到一个标量。

```python
v = torch.tensor([[1.0, 0.1], [0.01, 0.001]], dtype=torch.float)
z.backward(v)
print(x.grad)Copy to clipboardErrorCopied
```

输出：

```
tensor([2.0000, 0.2000, 0.0200, 0.0020])Copy to clipboardErrorCopied
```

注意，`x.grad`是和`x`同形的张量。

再来看看中断梯度追踪的例子：

```python
x = torch.tensor(1.0, requires_grad=True)
y1 = x ** 2 
with torch.no_grad():
    y2 = x ** 3
y3 = y1 + y2

print(x.requires_grad)
print(y1, y1.requires_grad) # True
print(y2, y2.requires_grad) # False
print(y3, y3.requires_grad) # TrueCopy to clipboardErrorCopied
```

输出：

```
True
tensor(1., grad_fn=<PowBackward0>) True
tensor(1.) False
tensor(2., grad_fn=<ThAddBackward>) TrueCopy to clipboardErrorCopied
```

可以看到，上面的`y2`是没有`grad_fn`而且`y2.requires_grad=False`的，而`y3`是有`grad_fn`的。如果我们将`y3`对`x`求梯度的话会是多少呢？

```python
y3.backward()
print(x.grad)Copy to clipboardErrorCopied
```

输出：

```
tensor(2.)Copy to clipboardErrorCopied
```

为什么是2呢？y3=y1+y2=x2+x3y_3 = y_1 + y_2 = x^2 + x^3*y*3=*y*1+*y*2=*x*2+*x*3，当 x=1x=1*x*=1 时 dy3dx\frac {dy_3} {dx}*d**x**d**y*3 不应该是5吗？事实上，由于 y2y_2*y*2 的定义是被`torch.no_grad():`包裹的，所以与 y2y_2*y*2 有关的梯度是不会回传的，只有与 y1y_1*y*1 有关的梯度才会回传，即 x2x^2*x*2 对 xx*x* 的梯度。

上面提到，`y2.requires_grad=False`，所以不能调用 `y2.backward()`，会报错：

```
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fnCopy to clipboardErrorCopied
```

此外，如果我们想要修改`tensor`的数值，但是又不希望被`autograd`记录（即不会影响反向传播），那么我么可以对`tensor.data`进行操作。

```python
x = torch.ones(1,requires_grad=True)

print(x.data) # 还是一个tensor
print(x.data.requires_grad) # 但是已经是独立于计算图之外

y = 2 * x
x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播

y.backward()
print(x) # 更改data的值也会影响tensor的值
print(x.grad)Copy to clipboardErrorCopied
```

输出：

```
tensor([1.])
False
tensor([100.], requires_grad=True)
tensor([2.])
```